# ğŸ  SGE-Replication â€” Spatial Group-wise Enhance in Convolutional Networks

This repository provides a **clean, forward-only PyTorch replication** of the
*Spatial Group-wise Enhance (SGE)* module proposed in  
*Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks*.

The purpose is **architectural and theoretical fidelity**: translating the paperâ€™s
**group-wise semantic assumption, similarity-based attention formulation, and
lightweight design** into minimal, readable code â€” **without training, datasets,
or benchmark evaluation**.

The focus is strictly on how **semantic sub-features within channel groups**
can **self-enhance their spatial distributions** using globalâ€“local similarity,
rather than channel competition or heavy attention mechanisms âŸ.

Paper reference:  [Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks (arXiv 2019)](https://arxiv.org/abs/1905.09646)

---

## Overview â€” Semantic Groups That Organize Themselves â—ˆ

![SGE Overview](images/figmix.jpg)

>Convolutional feature maps implicitly contain multiple semantic entities
>distributed across channels. These entities often appear as **groups of
>sub-features**, each corresponding to object parts or high-level concepts.

However, due to noisy backgrounds and weak spatial supervision, these grouped
features tend to activate **incoherently across space**, leading to ambiguous
localization and diluted semantic responses.

SGE addresses this by introducing a **spatial self-enhancement mechanism inside
each semantic group**, allowing every group to autonomously:

- Strengthen spatially consistent semantic regions â—Œ
- Suppress irrelevant or noisy activations â—
- Preserve lightweight computation and parameter efficiency â—

Crucially, this enhancement is guided **only by the similarity between global
and local descriptors within each group**, without cross-group interaction or
external supervision.

---

## Spatial Group-wise Enhance Formulation ğŸ§®

Consider a convolutional feature map

$$
X \in \mathbb{R}^{C \times H \times W}
$$

which is divided into $G$ groups along the channel dimension.  
Each group contains $C/G$ channels and defines a set of spatial vectors:

$$
X_g = \{ x_1, x_2, \dots, x_m \}, \quad x_i \in \mathbb{R}^{C/G}, \quad m = H \times W
$$

### Global Semantic Descriptor

For each group, a global semantic vector is obtained via spatial averaging:

$$
g = \frac{1}{m} \sum_{i=1}^{m} x_i
$$

This vector approximates the **semantic entity** represented by the group.

---

### Similarity-based Spatial Attention

Each spatial location is assigned an importance coefficient via dot-product
similarity:

$$
c_i = g \cdot x_i
$$

These coefficients are normalized across spatial positions:

$$
\hat{c}_i = \frac{c_i - \mu_c}{\sigma_c + \varepsilon}
$$

and transformed using learnable scale and shift parameters:

$$
a_i = \gamma \hat{c}_i + \beta
$$

Finally, the original feature vector is enhanced through a sigmoid gate:

$$
\hat{x}_i = x_i \cdot \sigma(a_i)
$$

This operation selectively amplifies spatial locations that are **globally
consistent with the groupâ€™s semantic identity**, while compressing noise.

---

## Architectural Interpretation â—¬

- Enhancement operates **independently inside each group**
- No channel competition or cross-group attention
- Only **$2G$ learnable parameters** ($\gamma, \beta$ per group)
- No additional convolutions or heavy context modeling
- Fully compatible with residual CNN backbones

SGE is typically inserted **after the final BatchNorm layer** in a residual
bottleneck, mirroring standard integration practices.

Despite its simplicity, the mechanism yields **sharper spatial contrast** and
more interpretable semantic activation patterns across network stages.

---

## Repository Structure ğŸ—ƒï¸

```bash
SGE-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ visualization.py    
â”‚   â”‚   â””â”€â”€ activation_stats.py  
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â””â”€â”€ sge.py              
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â””â”€â”€ residual_sge.py       
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ backbone_stub.py     
â”‚   â”‚   â””â”€â”€ sgenet_stub.py       
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline.py             
â”‚   â””â”€â”€ config.py                 
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                
â”‚
â”œâ”€â”€ requirements.txt               
â””â”€â”€ README.md                      
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
